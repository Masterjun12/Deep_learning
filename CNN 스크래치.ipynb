{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SdZQcfLRHaXZ",
        "outputId": "68b464b1-b655-453a-cf75-7e6d0591bb16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1 \t batch: 1 \t loss: \t 3.8959405772009843\t정확도: 12.183333333333334\n",
            "epoch:1 \t batch: 2 \t loss: \t 3.438017324428408\t정확도: 14.716666666666667\n",
            "epoch:1 \t batch: 3 \t loss: \t 3.235658805048187\t정확도: 13.883333333333333\n",
            "epoch:1 \t batch: 4 \t loss: \t 3.169565733327532\t정확도: 15.483333333333333\n",
            "epoch:1 \t batch: 5 \t loss: \t 3.135273567322882\t정확도: 18.71666666666667\n",
            "epoch:1 \t batch: 6 \t loss: \t 3.090188263557399\t정확도: 23.333333333333332\n",
            "epoch:1 \t batch: 7 \t loss: \t 3.0370884521471946\t정확도: 26.8\n",
            "epoch:1 \t batch: 8 \t loss: \t 2.9906753231094196\t정확도: 30.516666666666666\n",
            "epoch:1 \t batch: 9 \t loss: \t 2.937120481608016\t정확도: 33.31666666666667\n",
            "epoch:1 \t batch: 10 \t loss: \t 2.8417386460064322\t정확도: 37.71666666666667\n",
            "epoch:2 \t batch: 1 \t loss: \t 2.759748214273094\t정확도: 39.61666666666667\n",
            "epoch:2 \t batch: 2 \t loss: \t 2.682421406582395\t정확도: 40.61666666666667\n",
            "epoch:2 \t batch: 3 \t loss: \t 2.6227671174513256\t정확도: 40.31666666666667\n",
            "epoch:2 \t batch: 4 \t loss: \t 2.528125308909367\t정확도: 42.88333333333333\n",
            "epoch:2 \t batch: 5 \t loss: \t 2.504477827519902\t정확도: 41.766666666666666\n",
            "epoch:2 \t batch: 6 \t loss: \t 2.796952902934953\t정확도: 35.18333333333333\n",
            "epoch:2 \t batch: 7 \t loss: \t 3.1161130333318154\t정확도: 34.21666666666667\n",
            "epoch:2 \t batch: 8 \t loss: \t 2.6285158598675897\t정확도: 38.666666666666664\n",
            "epoch:2 \t batch: 9 \t loss: \t 2.3654445502696864\t정확도: 46.800000000000004\n",
            "epoch:2 \t batch: 10 \t loss: \t 2.1622433574787627\t정확도: 52.666666666666664\n",
            "epoch:3 \t batch: 1 \t loss: \t 2.0936856164212485\t정확도: 54.949999999999996\n",
            "epoch:3 \t batch: 2 \t loss: \t 2.0919404358406926\t정확도: 52.5\n",
            "epoch:3 \t batch: 3 \t loss: \t 2.5232455543706287\t정확도: 42.733333333333334\n",
            "epoch:3 \t batch: 4 \t loss: \t 3.5123711728834097\t정확도: 33.86666666666667\n",
            "epoch:3 \t batch: 5 \t loss: \t 3.079365600678638\t정확도: 28.299999999999997\n",
            "epoch:3 \t batch: 6 \t loss: \t 2.755588131064503\t정확도: 35.66666666666667\n",
            "epoch:3 \t batch: 7 \t loss: \t 2.542165383309151\t정확도: 42.15\n",
            "epoch:3 \t batch: 8 \t loss: \t 2.3702210806855772\t정확도: 50.63333333333333\n",
            "epoch:3 \t batch: 9 \t loss: \t 2.20154409845292\t정확도: 54.78333333333333\n",
            "epoch:3 \t batch: 10 \t loss: \t 2.026605868867329\t정확도: 58.35\n",
            "epoch:4 \t batch: 1 \t loss: \t 1.9479500265513177\t정확도: 58.96666666666667\n",
            "epoch:4 \t batch: 2 \t loss: \t 1.899568239374673\t정확도: 60.483333333333334\n",
            "epoch:4 \t batch: 3 \t loss: \t 2.2171274271986188\t정확도: 50.21666666666667\n",
            "epoch:4 \t batch: 4 \t loss: \t 2.9955667559720625\t정확도: 34.983333333333334\n",
            "epoch:4 \t batch: 5 \t loss: \t 2.598894419973591\t정확도: 44.166666666666664\n",
            "epoch:4 \t batch: 6 \t loss: \t 2.2668437446810157\t정확도: 45.766666666666666\n",
            "epoch:4 \t batch: 7 \t loss: \t 2.1226637691099612\t정확도: 48.88333333333333\n",
            "epoch:4 \t batch: 8 \t loss: \t 2.0284648770490095\t정확도: 51.38333333333334\n",
            "epoch:4 \t batch: 9 \t loss: \t 1.907588991246886\t정확도: 57.36666666666667\n",
            "epoch:4 \t batch: 10 \t loss: \t 1.7009942490978531\t정확도: 63.733333333333334\n",
            "epoch:5 \t batch: 1 \t loss: \t 1.660959410367435\t정확도: 64.56666666666668\n",
            "epoch:5 \t batch: 2 \t loss: \t 1.733914674592064\t정확도: 61.36666666666667\n",
            "epoch:5 \t batch: 3 \t loss: \t 2.518884907036152\t정확도: 46.88333333333333\n",
            "epoch:5 \t batch: 4 \t loss: \t 2.664657902219491\t정확도: 46.666666666666664\n",
            "epoch:5 \t batch: 5 \t loss: \t 2.246078496586368\t정확도: 46.916666666666664\n",
            "epoch:5 \t batch: 6 \t loss: \t 2.0605323666046345\t정확도: 56.08333333333333\n",
            "epoch:5 \t batch: 7 \t loss: \t 1.7784364954142402\t정확도: 61.96666666666667\n",
            "epoch:5 \t batch: 8 \t loss: \t 1.6646037676254382\t정확도: 63.849999999999994\n",
            "epoch:5 \t batch: 9 \t loss: \t 1.5628201423243224\t정확도: 68.73333333333333\n",
            "epoch:5 \t batch: 10 \t loss: \t 1.406285013885416\t정확도: 71.15\n",
            "epoch:6 \t batch: 1 \t loss: \t 1.4395964923642672\t정확도: 70.33333333333334\n",
            "epoch:6 \t batch: 2 \t loss: \t 1.4547090635716586\t정확도: 69.26666666666667\n",
            "epoch:6 \t batch: 3 \t loss: \t 1.7846338668146937\t정확도: 60.46666666666667\n",
            "epoch:6 \t batch: 4 \t loss: \t 2.149930919347398\t정확도: 54.85\n",
            "epoch:6 \t batch: 5 \t loss: \t 2.6159263910854325\t정확도: 47.16666666666667\n",
            "epoch:6 \t batch: 6 \t loss: \t 2.7250627292768153\t정확도: 38.550000000000004\n",
            "epoch:6 \t batch: 7 \t loss: \t 2.0037420782251623\t정확도: 53.449999999999996\n",
            "epoch:6 \t batch: 8 \t loss: \t 1.7218196680432072\t정확도: 62.26666666666667\n",
            "epoch:6 \t batch: 9 \t loss: \t 1.5202162105544215\t정확도: 69.1\n",
            "epoch:6 \t batch: 10 \t loss: \t 1.3180888670420403\t정확도: 74.61666666666666\n",
            "epoch:7 \t batch: 1 \t loss: \t 1.3344750694836525\t정확도: 73.53333333333333\n",
            "epoch:7 \t batch: 2 \t loss: \t 1.2830976973330972\t정확도: 74.58333333333333\n",
            "epoch:7 \t batch: 3 \t loss: \t 1.4350846025305815\t정확도: 69.93333333333334\n",
            "epoch:7 \t batch: 4 \t loss: \t 1.470126925038407\t정확도: 68.7\n",
            "epoch:7 \t batch: 5 \t loss: \t 1.7687361090805176\t정확도: 62.74999999999999\n",
            "epoch:7 \t batch: 6 \t loss: \t 1.8943859621884385\t정확도: 60.86666666666667\n",
            "epoch:7 \t batch: 7 \t loss: \t 1.5596595650687997\t정확도: 66.56666666666666\n",
            "epoch:7 \t batch: 8 \t loss: \t 1.39495176648578\t정확도: 72.16666666666667\n",
            "epoch:7 \t batch: 9 \t loss: \t 1.2822136731242681\t정확도: 74.7\n",
            "epoch:7 \t batch: 10 \t loss: \t 1.1618732948457653\t정확도: 76.88333333333334\n",
            "epoch:8 \t batch: 1 \t loss: \t 1.2519298058661577\t정확도: 73.58333333333333\n",
            "epoch:8 \t batch: 2 \t loss: \t 1.340073724143948\t정확도: 71.53333333333333\n",
            "epoch:8 \t batch: 3 \t loss: \t 1.618999115927559\t정확도: 66.08333333333334\n",
            "epoch:8 \t batch: 4 \t loss: \t 1.6329804630435076\t정확도: 64.46666666666667\n",
            "epoch:8 \t batch: 5 \t loss: \t 1.480098731212215\t정확도: 66.91666666666667\n",
            "epoch:8 \t batch: 6 \t loss: \t 1.4927343766262984\t정확도: 68.16666666666666\n",
            "epoch:8 \t batch: 7 \t loss: \t 1.385357230401501\t정확도: 69.81666666666668\n",
            "epoch:8 \t batch: 8 \t loss: \t 1.3768250781739315\t정확도: 70.83333333333334\n",
            "epoch:8 \t batch: 9 \t loss: \t 1.4536155583214483\t정확도: 68.43333333333334\n",
            "epoch:8 \t batch: 10 \t loss: \t 1.3034775000134735\t정확도: 72.91666666666666\n",
            "epoch:9 \t batch: 1 \t loss: \t 1.276033421742795\t정확도: 73.15\n",
            "epoch:9 \t batch: 2 \t loss: \t 1.2377254584251263\t정확도: 75.06666666666668\n",
            "epoch:9 \t batch: 3 \t loss: \t 1.363773874904161\t정확도: 71.39999999999999\n",
            "epoch:9 \t batch: 4 \t loss: \t 1.2187104942609368\t정확도: 74.65\n",
            "epoch:9 \t batch: 5 \t loss: \t 1.259517612441137\t정확도: 73.23333333333333\n",
            "epoch:9 \t batch: 6 \t loss: \t 1.434798889048454\t정확도: 69.13333333333334\n",
            "epoch:9 \t batch: 7 \t loss: \t 1.5365120666071646\t정확도: 66.4\n",
            "epoch:9 \t batch: 8 \t loss: \t 1.5138693022509417\t정확도: 68.25\n",
            "epoch:9 \t batch: 9 \t loss: \t 1.3110034908382224\t정확도: 72.76666666666667\n",
            "epoch:9 \t batch: 10 \t loss: \t 1.0566543008651808\t정확도: 79.80000000000001\n",
            "epoch:10 \t batch: 1 \t loss: \t 1.0834153817837893\t정확도: 77.93333333333334\n",
            "epoch:10 \t batch: 2 \t loss: \t 1.0896388624310154\t정확도: 79.06666666666666\n",
            "epoch:10 \t batch: 3 \t loss: \t 1.262303054822439\t정확도: 74.11666666666666\n",
            "epoch:10 \t batch: 4 \t loss: \t 1.2634054240369141\t정확도: 74.21666666666667\n",
            "epoch:10 \t batch: 5 \t loss: \t 1.3754713274279202\t정확도: 69.78333333333333\n",
            "epoch:10 \t batch: 6 \t loss: \t 1.592104947153241\t정확도: 66.66666666666666\n",
            "epoch:10 \t batch: 7 \t loss: \t 1.438796681461857\t정확도: 69.28333333333333\n",
            "epoch:10 \t batch: 8 \t loss: \t 1.2845662166309577\t정확도: 74.03333333333333\n",
            "epoch:10 \t batch: 9 \t loss: \t 1.1003954683166297\t정확도: 79.14999999999999\n",
            "epoch:10 \t batch: 10 \t loss: \t 0.9151590376551096\t정확도: 82.8\n",
            "epoch:11 \t batch: 1 \t loss: \t 0.9882967827762446\t정확도: 80.53333333333333\n",
            "epoch:11 \t batch: 2 \t loss: \t 1.0091498905577894\t정확도: 80.11666666666667\n",
            "epoch:11 \t batch: 3 \t loss: \t 1.2467771181053837\t정확도: 73.66666666666667\n",
            "epoch:11 \t batch: 4 \t loss: \t 1.2062260807532597\t정확도: 75.16666666666667\n",
            "epoch:11 \t batch: 5 \t loss: \t 1.3571081296474246\t정확도: 71.89999999999999\n",
            "epoch:11 \t batch: 6 \t loss: \t 1.2195332268983756\t정확도: 75.11666666666666\n",
            "epoch:11 \t batch: 7 \t loss: \t 1.035734702620802\t정확도: 79.86666666666666\n",
            "epoch:11 \t batch: 8 \t loss: \t 1.0433552438365716\t정확도: 79.5\n",
            "epoch:11 \t batch: 9 \t loss: \t 1.0681265229779096\t정확도: 78.73333333333333\n",
            "epoch:11 \t batch: 10 \t loss: \t 1.0348366728617813\t정확도: 78.93333333333334\n",
            "epoch:12 \t batch: 1 \t loss: \t 1.2208592759520671\t정확도: 74.4\n",
            "epoch:12 \t batch: 2 \t loss: \t 1.3532453352308838\t정확도: 72.39999999999999\n",
            "epoch:12 \t batch: 3 \t loss: \t 1.255628084508411\t정확도: 72.86666666666667\n",
            "epoch:12 \t batch: 4 \t loss: \t 1.0065828118495097\t정확도: 80.9\n",
            "epoch:12 \t batch: 5 \t loss: \t 0.9547850963274895\t정확도: 81.21666666666667\n",
            "epoch:12 \t batch: 6 \t loss: \t 1.018774477929716\t정확도: 80.01666666666667\n",
            "epoch:12 \t batch: 7 \t loss: \t 0.9500611001124352\t정확도: 81.71666666666667\n",
            "epoch:12 \t batch: 8 \t loss: \t 1.0156866607787307\t정확도: 80.05\n",
            "epoch:12 \t batch: 9 \t loss: \t 1.0917660476340092\t정확도: 78.08333333333334\n",
            "epoch:12 \t batch: 10 \t loss: \t 1.2028535200402175\t정확도: 76.25\n",
            "epoch:13 \t batch: 1 \t loss: \t 1.3909044982071155\t정확도: 71.61666666666666\n",
            "epoch:13 \t batch: 2 \t loss: \t 1.5106949853954648\t정확도: 68.96666666666667\n",
            "epoch:13 \t batch: 3 \t loss: \t 1.2765922550431288\t정확도: 73.58333333333333\n",
            "epoch:13 \t batch: 4 \t loss: \t 1.0301585053855924\t정확도: 80.45\n",
            "epoch:13 \t batch: 5 \t loss: \t 0.9668894975362267\t정확도: 81.71666666666667\n",
            "epoch:13 \t batch: 6 \t loss: \t 1.034241076170629\t정확도: 80.66666666666666\n",
            "epoch:13 \t batch: 7 \t loss: \t 1.008820220271633\t정확도: 79.93333333333334\n",
            "epoch:13 \t batch: 8 \t loss: \t 1.0620315380588523\t정확도: 78.88333333333334\n",
            "epoch:13 \t batch: 9 \t loss: \t 1.1079582624073696\t정확도: 77.61666666666667\n",
            "epoch:13 \t batch: 10 \t loss: \t 1.0279964140725006\t정확도: 78.8\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1a2c52c13d6f>\u001b[0m in \u001b[0;36m<cell line: 369>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# 역전파 및 확률적 경사 하강법 적용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mcomplete_NN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_testing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mcomplete_NN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplying_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mii\u001b[0m  \u001b[0;31m# 다음 배치의 시작 인덱스로 이동\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1a2c52c13d6f>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, Y)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 각 레이어를 역순으로 통해 역전파 진행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# 확률적 경사 하강법(SGD)을 적용하는 메서드 정의\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1a2c52c13d6f>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, pooled)\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                     \u001b[0mcheated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpooled_transpose_re\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# 역전파 결과 갱신\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcheated\u001b[0m  \u001b[0;31m# 역전파 결과 반환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "fac = 5\n",
        "Mnist = tf.keras.datasets.mnist\n",
        "\n",
        "class Linear_Layer:\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, alpha=0.01, Theta=None, bias=None):\n",
        "        self.alpha = alpha  # 학습률(alpha) 초기화\n",
        "\n",
        "        # Theta(가중치 행렬)가 주어지지 않으면 무작위로 생성\n",
        "        if Theta is None:\n",
        "            self.Theta = np.random.randn(in_dim, out_dim) / fac  # 무작위 가중치 행렬 생성\n",
        "        else:\n",
        "            self.Theta = Theta  # 주어진 Theta 사용\n",
        "\n",
        "        # bias(편향 벡터)가 주어지지 않으면 무작위로 생성\n",
        "        if bias is None:\n",
        "            self.bias = np.random.randn(out_dim) / fac  # 무작위 편향 벡터 생성\n",
        "        else:\n",
        "            self.bias = bias  # 주어진 bias 사용\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, X):\n",
        "        self.X = X\n",
        "        self.z = np.matmul(X, self.Theta) + self.bias  # 선형 변환 계산\n",
        "        return self.z\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, grad_previous):\n",
        "        t = self.X.shape[0]  # 입력 데이터의 행 수\n",
        "        self.grad = np.matmul(self.X.transpose(), grad_previous) / t  # Theta에 대한 그래디언트 계산\n",
        "        self.grad_bias = grad_previous.sum(axis=0) / t  # 편향에 대한 그래디언트 계산\n",
        "        self.grad_a = np.matmul(grad_previous, self.Theta.transpose())  # 입력에 대한 그래디언트 계산\n",
        "        return self.grad_a\n",
        "\n",
        "    # 확률적 경사 하강법(SGD)을 적용하는 메서드 정의\n",
        "    def applying_sgd(self):\n",
        "        self.Theta = self.Theta - (self.alpha * self.grad)  # Theta 업데이트\n",
        "        self.bias = self.bias - (self.alpha * self.grad_bias)  # 편향 업데이트\n",
        "\n",
        "class softmax:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # 원-핫 인코딩을 수행하는 메서드\n",
        "    def expansion(self, t):\n",
        "        (a,) = t.shape  # 입력 벡터의 길이(a)를 가져옴\n",
        "        Y = np.zeros((a, 10))  # 10개의 클래스에 대한 원-핫 인코딩을 위한 빈 행렬 생성\n",
        "        for i in range(0, a):\n",
        "            Y[i, t[i]] = 1  # 입력 벡터의 각 원소에 대해 해당하는 클래스 인덱스를 1로 설정\n",
        "        return Y\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, z):\n",
        "        self.z = z  # 입력값 저장\n",
        "        (p, t) = self.z.shape  # p: 입력 데이터의 개수, t: 클래스 수\n",
        "        self.a = np.zeros((p, t))  # 출력값을 저장할 배열 초기화\n",
        "        for i in range(0, p):\n",
        "            for ii in range(0, t):\n",
        "                # 소프트맥스 함수를 이용하여 클래스 확률 계산\n",
        "                self.a[i, ii] = (np.exp(self.z[i, ii])) / (np.sum(np.exp(self.z[i, :])))\n",
        "        return self.a  # 계산된 클래스 확률 반환\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, Y):\n",
        "        y = self.expansion(Y)  # 실제 클래스를 원-핫 인코딩으로 변환\n",
        "        self.grad = (self.a - y)  # 그래디언트 계산\n",
        "        return self.grad  # 그래디언트 반환\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass  # 확률적 경사 하강법(SGD)\n",
        "\n",
        "class relu:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, z):\n",
        "        if len(z.shape) == 3:  # 입력이 3차원인 경우\n",
        "            z_temp = z.reshape((z.shape[0], z.shape[1] * z.shape[2]))  # 3차원 배열을 2차원으로 변환\n",
        "            z_temp_1 = self.forward_pass(z_temp)  # 2차원 배열에 대해 순전파를 재귀적으로 호출\n",
        "            self.a_1 = z_temp_1.reshape((z.shape[0], z.shape[1], z.shape[2]))  # 결과를 다시 3차원으로 변환\n",
        "            return self.a_1\n",
        "\n",
        "        else:  # 입력이 2차원인 경우\n",
        "            (p, t) = z.shape  # p: 데이터 개수, t: 특성 수\n",
        "            self.a = np.zeros((p, t))  # 출력값을 저장할 배열 초기화\n",
        "            for i in range(0, p):\n",
        "                for ii in range(0, t):\n",
        "                    self.a[i, ii] = max([0, z[i, ii]])  # ReLU 활성화 함수 적용\n",
        "            return self.a\n",
        "\n",
        "    # ReLU 함수의 도함수를 계산하는 메서드 정의\n",
        "    def derivative(self, a):\n",
        "        if a > 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, grad_previous):\n",
        "        if len(grad_previous.shape) == 3:  # 입력이 3차원인 경우\n",
        "            (d, p, t) = grad_previous.shape\n",
        "            self.grad = np.zeros((d, p, t))  # 출력값에 대한 그래디언트를 저장할 배열 초기화\n",
        "\n",
        "            for i in range(d):\n",
        "                for ii in range(p):\n",
        "                    for iii in range(t):\n",
        "                        # ReLU 함수의 도함수를 적용한 그래디언트 계산\n",
        "                        self.grad[i, ii, iii] = (grad_previous[i, ii, iii] * self.derivative(self.a_1[i, ii, iii]))\n",
        "\n",
        "            return self.grad\n",
        "\n",
        "        else:  # 입력이 2차원인 경우\n",
        "            (p, t) = grad_previous.shape\n",
        "            self.grad = np.zeros((p, t))  # 출력값에 대한 그래디언트를 저장할 배열 초기화\n",
        "            for i in range(p):\n",
        "                for ii in range(t):\n",
        "                    # ReLU 함수의 도함수를 적용한 그래디언트 계산\n",
        "                    self.grad[i, ii] = grad_previous[i, ii] * self.derivative(self.a[i, ii])\n",
        "            return self.grad\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass  # 확률적 경사 하강법(SGD)\n",
        "\n",
        "\n",
        "class padding():\n",
        "\n",
        "    def __init__(self, pad=1):\n",
        "        self.pad = pad\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, data):\n",
        "        # 입력 데이터 주위에 패딩을 추가하여 출력 데이터 생성\n",
        "        X = np.pad(data, ((0, 0), (self.pad, self.pad), (self.pad, self.pad)), 'constant', constant_values=0)\n",
        "        return X\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, y):\n",
        "        # 출력 데이터의 패딩을 제거하고 반환\n",
        "        return y[:, 1:(y.shape[1] - 1), 1:(y.shape[2] - 1)]\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass  # 확률적 경사 하강법(SGD)\n",
        "\n",
        "class Convolutional_Layer:\n",
        "    def __init__(self, filter_dim=3, stride=1, pad=1, alpha=0.01):\n",
        "        self.filter_dim = filter_dim\n",
        "        self.stride = stride\n",
        "        self.filter = np.random.randn(self.filter_dim, self.filter_dim)\n",
        "        self.filter = self.filter / self.filter.sum()\n",
        "        self.bias = np.random.rand() / 10\n",
        "        self.pad = pad\n",
        "        self.alpha = alpha\n",
        "\n",
        "    # 컨볼루션 연산을 수행하는 메서드 정의\n",
        "    def convolving(self, X, fil, dimen_x, dimen_y):\n",
        "        z = np.zeros((dimen_x, dimen_y))\n",
        "        for i in range(dimen_x):\n",
        "            for ii in range(dimen_y):\n",
        "                temp = np.multiply(X[i: i + fil.shape[0], ii: ii + fil.shape[1]], fil)\n",
        "                z[i, ii] = temp.sum()\n",
        "        return z\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, X):\n",
        "        self.X = X\n",
        "        (d, p, t) = self.X.shape\n",
        "        dimen_x = int(((p - self.filter_dim) / self.stride) + 1)\n",
        "        dimen_y = int(((t - self.filter_dim) / self.stride) + 1)\n",
        "        self.z = np.zeros((d, dimen_x, dimen_y))\n",
        "        for i in range(d):\n",
        "            self.z[i] = (self.convolving(self.X[i], self.filter, dimen_x, dimen_y) + self.bias)\n",
        "\n",
        "        return self.z\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, grad_z):\n",
        "        (d, p, t) = grad_z.shape\n",
        "        filter_1 = np.flip((np.flip(self.filter, axis=0)), axis=1)\n",
        "        self.grads = np.zeros((d, p, t))\n",
        "        for i in range(d):\n",
        "            self.grads[i] = self.convolving(np.pad(grad_z[i], ((1, 1), (1, 1)), 'constant', constant_values=0),\n",
        "                                            filter_1, p, t)\n",
        "\n",
        "        self.grads = np.pad(self.grads, ((0, 0), (1, 1), (1, 1)), 'constant', constant_values=0)\n",
        "\n",
        "        self.grad_filter = np.zeros((self.filter_dim, self.filter_dim))\n",
        "\n",
        "        for i in range(self.filter_dim):\n",
        "            for ii in range(self.filter_dim):\n",
        "                self.grad_filter[i, ii] = (np.multiply(grad_z, self.X[:, i:p + i, ii:t + ii])).sum()\n",
        "        self.grad_filter = self.grad_filter / (d)\n",
        "\n",
        "        self.grad_bias = (grad_z.sum()) / (d)\n",
        "        return self.grads\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        # 확률적 경사 하강법(SGD)을 사용하여 가중치와 편향 업데이트\n",
        "        self.filter = self.filter - (self.alpha * self.grad_filter)\n",
        "        self.bias = self.bias - (self.alpha * self.grad_bias)\n",
        "\n",
        "class pooling:\n",
        "\n",
        "    def __init__(self, pool_dim=2, stride=2):\n",
        "        self.pool_dim = pool_dim  # 풀링 윈도우의 크기\n",
        "        self.stride = stride  # 스트라이드 값\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, data):\n",
        "        (q, p, t) = data.shape  # 입력 데이터의 형태\n",
        "        z_x = int((p - self.pool_dim) / self.stride) + 1  # x 축에 대한 풀링 결과 크기\n",
        "        z_y = int((t - self.pool_dim) / self.stride) + 1  # y 축에 대한 풀링 결과 크기\n",
        "        after_pool = np.zeros((q, z_x, z_y))  # 풀링 결과를 저장할 배열 초기화\n",
        "\n",
        "        for ii in range(0, q):  # 배치 개수만큼 반복\n",
        "            liss = []  # 풀링 결과를 저장할 리스트 초기화\n",
        "            for i in range(0, p, self.stride):\n",
        "                for j in range(0, t, self.stride):\n",
        "                    if (i + self.pool_dim <= p) and (j + self.pool_dim <= t):\n",
        "                        temp = data[ii, i:(i + self.pool_dim), j:(j + self.pool_dim)]\n",
        "                        temp_1 = np.max(temp)  # 최대값 풀링 연산 수행\n",
        "                        liss.append(temp_1)  # 결과를 리스트에 추가\n",
        "            liss = np.asarray(liss)  # 리스트를 NumPy 배열로 변환\n",
        "            liss = liss.reshape((z_x, z_y))  # 결과를 풀링 크기에 맞게 재구성\n",
        "            after_pool[ii] = liss  # 풀링 결과를 저장\n",
        "\n",
        "        return after_pool  # 풀링 결과 반환\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, pooled):\n",
        "        (a, b, c) = pooled.shape  # 풀링 결과의 형태\n",
        "        cheated = np.zeros((a, 2 * b, 2 * c))  # 역전파 결과를 저장할 배열 초기화\n",
        "\n",
        "        for k in range(0, a):  # 배치 개수만큼 반복\n",
        "            pooled_transpose_re = pooled[k].reshape((b * c))  # 풀링 결과를 벡터 형태로 재구성\n",
        "            count = 0  # 역전파 결과를 저장할 인덱스 초기화\n",
        "            for i in range(0, 2 * b, self.stride):\n",
        "                for j in range(0, 2 * c, self.stride):\n",
        "                    cheated[k, i:(i + self.stride), j:(j + self.stride)] = pooled_transpose_re[count]\n",
        "                    count = count + 1  # 역전파 결과 갱신\n",
        "\n",
        "        return cheated  # 역전파 결과 반환\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass  # 확률적 경사 하강법(SGD)\n",
        "\n",
        "class Neural_Network:\n",
        "\n",
        "    def __init__(self, Network):\n",
        "        self.Network = Network  # 네트워크 레이어들을 리스트로 받아 초기화\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, X):\n",
        "        n = X\n",
        "        for i in self.Network:\n",
        "            n = i.forward_pass(n)  # 각 레이어를 통해 순전파 진행\n",
        "\n",
        "        return n  # 최종 결과 반환\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, Y):\n",
        "        m = Y\n",
        "        for i in reversed(self.Network):\n",
        "            m = i.backprop(m)  # 각 레이어를 역순으로 통해 역전파 진행\n",
        "\n",
        "    # 확률적 경사 하강법(SGD)을 적용하는 메서드 정의\n",
        "    def applying_sgd(self):\n",
        "        for i in self.Network:\n",
        "            i.applying_sgd()  # 각 레이어에 대해 SGD 적용\n",
        "\n",
        "class reshaping:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # 순전파 메서드 정의\n",
        "    def forward_pass(self, a):\n",
        "        self.shape_a = a.shape  # 입력 데이터의 형태(shape) 저장\n",
        "\n",
        "        # 입력 데이터를 2차원 형태로 재구성하여 반환\n",
        "        self.final_a = a.reshape(self.shape_a[0], self.shape_a[1] * self.shape_a[2])\n",
        "        return self.final_a\n",
        "\n",
        "    # 역전파 메서드 정의\n",
        "    def backprop(self, q):\n",
        "        # 역전파 시에는 입력 데이터의 형태를 원래 형태로 재구성하여 반환\n",
        "        return q.reshape(self.shape_a[0], self.shape_a[1], self.shape_a[2])\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass  # 확률적 경사 하강법(SGD)\n",
        "\n",
        "class cross_entropy:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # 원-핫 인코딩을 수행하는 메서드\n",
        "    def expansion(self, t):\n",
        "        (a,) = t.shape\n",
        "        Y = np.zeros((a, 10))  # 10개의 클래스에 대한 원-핫 인코딩을 위한 빈 행렬 생성\n",
        "        for i in range(0, a):\n",
        "            Y[i, t[i]] = 1  # 입력 벡터의 각 원소에 대해 해당하는 클래스 인덱스를 1로 설정\n",
        "        return Y\n",
        "\n",
        "    # 크로스 엔트로피 손실을 계산하는 메서드\n",
        "    def loss(self, A, Y):\n",
        "        exp_Y = self.expansion(Y)  # 실제 클래스를 원-핫 인코딩으로 변환\n",
        "        (u, i) = A.shape  # u: 데이터 개수, i: 클래스 수\n",
        "        loss_matrix = np.zeros((u, i))  # 손실 값을 저장할 배열 초기화\n",
        "        for j in range(u):\n",
        "            for jj in range(i):\n",
        "                if exp_Y[j, jj] == 0:\n",
        "                    loss_matrix[j, jj] = np.log(1 - A[j, jj])\n",
        "                else:\n",
        "                    loss_matrix[j, jj] = np.log(A[j, jj])\n",
        "\n",
        "        # 전체 손실의 평균을 계산하여 반환\n",
        "        return ((-(loss_matrix.sum())) / u)\n",
        "\n",
        "class accuracy:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # 정확도를 계산하는 메서드\n",
        "    def value(self, out, Y):\n",
        "        self.out = np.argmax(out, axis=1)  # 출력값(out)에서 가장 큰 값의 인덱스를 가져옴\n",
        "        p = self.out.shape[0]  # 데이터 개수\n",
        "        total = 0  # 정확하게 예측된 데이터 수를 저장하는 변수\n",
        "        for i in range(p):\n",
        "            if Y[i] == self.out[i]:  # 실제 클래스(Y)와 예측 클래스(self.out)가 일치하면\n",
        "                total += 1  # 정확하게 예측된 데이터 수를 증가\n",
        "        return total / p  # 정확도를 전체 데이터 개수로 나누어 반환\n",
        "\n",
        "(Xtr, Ytr), (Xte, Yte) = Mnist.load_data()\n",
        "X_testing = Xtr[:, :, :]\n",
        "Y_testing = Ytr[:]\n",
        "X_testing = X_testing/255  # 입력 데이터를 0과 1 사이로 스케일링\n",
        "al = 0.3  # 학습률(learning rate)\n",
        "stopper = 85.0  # 정확도가 이 값 이상이면 훈련 중단\n",
        "\n",
        "complete_NN = Neural_Network([\n",
        "\n",
        "                                padding(),\n",
        "                                Convolutional_Layer(),\n",
        "                                pooling(),\n",
        "                                relu(),\n",
        "                                padding(),\n",
        "                                Convolutional_Layer(),\n",
        "                                pooling(),\n",
        "                                relu(),\n",
        "                                reshaping(),\n",
        "                                Linear_Layer(7*7, 24, alpha = al), #선형 레이어 - 7x7 크기의 입력을 24개의 출력으로 변환합니다.\n",
        "                                relu(),\n",
        "                                Linear_Layer(24, 10, alpha = al), #또 다른 선형 레이어 - 24개의 입력을 10개의 출력으로 변환합니다.\n",
        "                                softmax()\n",
        "\n",
        "                                ])\n",
        "CE = cross_entropy() #크로스 엔트로피 손실 함수\n",
        "\n",
        "acc = accuracy()  # 정확도 계산 객체 생성\n",
        "epochs = 100  # 총 에포크 수\n",
        "broke = 0  # 정확도가 stopper 이상인 경우 훈련 중단 플래그\n",
        "batches = 6000  # 각 배치의 크기\n",
        "\n",
        "for i in range(epochs):  # 에포크 반복\n",
        "    k = 0  # 데이터 인덱스 초기화\n",
        "    for ii in range(batches, 60001, batches):  # 배치 크기로 데이터를 나누어 처리\n",
        "\n",
        "        # 순전파 수행하여 손실 및 정확도 계산\n",
        "        out = complete_NN.forward_pass(X_testing[k:ii])\n",
        "        print(\"epoch:{} \\t batch: {} \\t loss: \\t {}\".format(i+1, int(ii/batches), CE.loss(out, Y_testing[k:ii])), end=\"\\t\")\n",
        "        accur = acc.value(out, Y_testing[k:ii])*100\n",
        "        print(\"정확도: {}\".format(accur))\n",
        "\n",
        "        if accur >= stopper:  # 정확도가 stopper 이상이면 훈련 중단\n",
        "            broke = 1\n",
        "            break\n",
        "\n",
        "        # 역전파 및 확률적 경사 하강법 적용\n",
        "        complete_NN.backprop(Y_testing[k:ii])\n",
        "        complete_NN.applying_sgd()\n",
        "        k = ii  # 다음 배치의 시작 인덱스로 이동\n",
        "\n",
        "    if broke == 1:  # 정확도가 stopper 이상인 경우 훈련 중단\n",
        "        break\n",
        "\n",
        "# 훈련된 신경망 모델을 사용하여 훈련 데이터에 대한 결과 계산\n",
        "out = complete_NN.forward_pass(X_testing)\n",
        "\n",
        "# 훈련 데이터에 대한 최종 손실을 계산하고 출력\n",
        "print(\"트레인 로스 {}\".format(CE.loss(out, Y_testing)))\n",
        "\n",
        "# 훈련 데이터에 대한 정확도를 계산하고 출력\n",
        "print(\"트레인 정확도 {}\".format(acc.value(out, Y_testing)*100))\n",
        "\n",
        "# 테스트 데이터를 전처리 (스케일링)\n",
        "Xtest = Xte/255\n",
        "\n",
        "# 테스트 데이터에 대한 결과 계산\n",
        "out_1 = complete_NN.forward_pass(Xtest)\n",
        "\n",
        "# 테스트 데이터에 대한 정확도를 계산하고 출력\n",
        "print(\"테스트 정확도 {}\".format(acc.value(out_1, Yte)*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# 랜덤한 인덱스 선택\n",
        "random_index = np.random.randint(0, len(x_test))\n",
        "random_image = x_test[random_index]\n",
        "random_label = y_test[random_index]\n",
        "\n",
        "# 선택한 이미지를 모델에 입력하기 위해 전처리\n",
        "input_image = random_image.reshape(1, 28, 28) / 255.0  # 이미지 스케일링 및 형태 조정\n",
        "\n",
        "# 모델 예측\n",
        "predictions = complete_NN.forward_pass(input_image)\n",
        "\n",
        "# 예측 결과에서 가장 높은 확률을 갖는 클래스 선택\n",
        "predicted_label = np.argmax(predictions)\n",
        "\n",
        "# 이미지 출력\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(random_image, cmap='gray')\n",
        "plt.title(f\"True Label: {random_label}, Predicted Label: {predicted_label}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "1TZBXCDMTFnM",
        "outputId": "e1a85ac9-43e8-42f9-e45d-9f8f9f6e3631"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAFeCAYAAADnm4a1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZEklEQVR4nO3deXDU9f3H8dcSMJeIgYRTTSDciMUm4lEhaHQCFTEIZtKK3CgtCDqjFjoqMirYqi0KWERAOWwZBHQcB6HIUXHkUHA4RUETFEFJjNHIFch+fn8w2Z9h84ZPEkIAn48ZZsyX924+u1me+e7u9+sGnHNOAIAwtWp6AQBwriKQAGAgkABgIJAAYCCQAGAgkABgIJAAYCCQAGAgkABgIJBn0RNPPKFAIKD8/Pwzdp0DBw5UUlLSGbu+C8Hq1asVCAS0evXq0LZz7X4qb43VLTc3V4FAQM8999wZu86auB1nU40FMhAIeP2p6Tu+W7duuvLKK2t0DdXpwQcf1G9/+1vVr19fMTExateunZ544gn9/PPPlb7Obt26lfkZ1q9fX9dcc41mzZqlYDB4Bldf/SZMmKC33nqrxr7/a6+9pkAgoI8//rjG1lCd3nzzTWVkZKhp06aKjIzUZZddpr59+2rbtm01vTRJUu2a+sZz584t8/WcOXO0fPnysO3t2rU7m8v61fnoo4/UpUsXDRo0SFFRUfrkk0/0zDPP6L333tP777+vWrUq9zv0sssu08SJEyVJeXl5mjNnjoYMGaLPP/9czzzzzJm8CV5eeeWVSsV5woQJ6tu3rzIzM8/8oqCtW7cqLi5Oo0ePVnx8vL799lvNmjVLnTt31tq1a/Wb3/ymRtdXY4Hs169fma/XrVun5cuXh20/2aFDhxQTE1OdS/tV+eCDD8K2JScn66GHHtKGDRt03XXXVep669WrV+Zned9996lNmzaaMmWKnnzySdWpUyfsMsFgUMXFxYqKiqrU9zyV8r4fat7jjz8etm3o0KG67LLL9K9//UvTpk2rgVX9v3P6NcjSp7cbN25U165dFRMTo7/+9a+STjxFf+KJJ8Iuk5SUpIEDB5bZVlhYqAceeECXX365IiMj1bJlS/3tb387Y0/3tmzZooEDB6pFixaKiopS48aNNXjwYH3//fflzufn5ysrK0uXXHKJGjRooNGjR+vIkSNhc/PmzVNKSoqio6NVv359ZWdn6+uvvz7tevbv36+dO3fq2LFjlbo9pa/VFRYWVury5YmJidF1112ngwcPKi8vT9KJn+HIkSP1+uuvq0OHDoqMjNTSpUslSd98840GDx6sRo0aKTIyUh06dNCsWbPCrnfv3r3KzMxUbGysGjZsqAcffFBHjx4NmyvvNchgMKgXXnhBHTt2VFRUlBISEtS9e/fQ09lAIKCDBw9q9uzZoZcLfvnYOtNrrKzi4mI9/vjjSklJUb169RQbG6suXbpo1apV5mX++c9/KjExUdHR0UpLSyv3Ke3OnTvVt29f1a9fX1FRUUpNTdXbb7992vUcOnRIO3furPRr7Q0bNlRMTMwZffxVVo3tQfr6/vvv1aNHD2VnZ6tfv35q1KhRhS5/6NAhpaWl6ZtvvtF9992nK664Qh9++KHGjh2r/fv3a9KkSVVe4/Lly/Xll19q0KBBaty4sbZv367p06dr+/btWrdunQKBQJn5rKwsJSUlaeLEiVq3bp1efPFF/fDDD5ozZ05o5umnn9Zjjz2mrKwsDR06VHl5eZo8ebK6du2qTz75RJdeeqm5nrFjx2r27NnKycnxemPi+PHjKiwsVHFxsbZt26ZHH31UdevWVefOnSt7l5Tryy+/VERERJm1r1y5UgsWLNDIkSMVHx+vpKQkfffdd7ruuutCAU1ISNC7776rIUOG6KefftIDDzwgSTp8+LDS09P11VdfadSoUWratKnmzp2rlStXeq1nyJAheu2119SjRw8NHTpUx48f15o1a7Ru3TqlpqZq7ty5Gjp0qDp37qx7771X0om9a0lnbY0+fvrpJ82YMUN/+MMfNGzYMBUVFWnmzJnKyMjQhg0b1KlTpzLzc+bMUVFRkUaMGKEjR47ohRde0M0336ytW7eG/n1t375dv/vd79SsWTONGTNGsbGxWrBggTIzM7Vo0SL17t3bXM+GDRt00003ady4ceXuxJSnsLBQx44d07fffqtJkybpp59+Unp6emXvkjPHnSNGjBjhTl5OWlqak+SmTZsWNi/JjRs3Lmx7YmKiGzBgQOjrJ5980sXGxrrPP/+8zNyYMWNcRESE++qrr065rrS0NNehQ4dTzhw6dChs23/+8x8nyb3//vuhbePGjXOSXK9evcrM/vnPf3aS3ObNm51zzuXm5rqIiAj39NNPl5nbunWrq127dpntAwYMcImJiWXmBgwY4CS5nJycU6671Nq1a52k0J82bdq4VatWeV22PGlpaa5t27YuLy/P5eXluU8//dSNGjXKSXK33357aE6Sq1Wrltu+fXuZyw8ZMsQ1adLE5efnl9menZ3t6tWrF7q/J02a5CS5BQsWhGYOHjzoWrZs6SSVuQ0n308rV650ktyoUaPC1h8MBkP/HRsbW+bxVJ1rLM+rr77qJLmPPvrInDl+/Lg7evRomW0//PCDa9SokRs8eHBoW05OjpPkoqOj3d69e0Pb169f7yS5Bx98MLQtPT3ddezY0R05ciS0LRgMuhtuuMG1atUqtG3VqlVht6N0W3n/Pi1t2rQJPf4uvvhi9+ijj7qSkhLvy1eXc/optiRFRkZq0KBBlb78G2+8oS5duiguLk75+fmhP7fccotKSkr0/vvvV3mN0dHRof8+cuSI8vPzQ6/dbdq0KWx+xIgRZb6+//77JUlLliyRJC1evFjBYFBZWVll1ty4cWO1atXqlE+dpBPvfDrnvA9rad++vZYvX6633npLjzzyiGJjY6v0LrZ04ulZQkKCEhIS1K5dO02ePFm33XZb2FPQtLQ0tW/fPvS1c06LFi3S7bffLudcmdufkZGhH3/8MXSfLlmyRE2aNFHfvn1Dl4+JiQnt7Z3KokWLFAgENG7cuLC/O3mP/2Rna42+IiIidNFFF0k68bJBQUGBjh8/rtTU1HIff5mZmWrWrFno686dO+vaa68NPf4KCgq0cuVKZWVlqaioKHTbvv/+e2VkZGjXrl365ptvzPV069ZNzjnvvUdJevXVV7V06VK99NJLateunQ4fPqySkhLvy1eXc/4pdrNmzUI//MrYtWuXtmzZooSEhHL//sCBA5W+7lIFBQUaP3685s+fH3Z9P/74Y9h8q1atynydnJysWrVqKTc3N7Rm51zYXKkz/YbDJZdcoltuuUWSdMcdd+jf//637rjjDm3atKnS7yImJSXplVdeUSAQUFRUlFq1aqWGDRuGzTVv3rzM13l5eSosLNT06dM1ffr0cq+79D7es2ePWrZsGRa0Nm3anHZ9X3zxhZo2bar69ev73qSzvsaKmD17tp5//vmw155Pvn+l8MefJLVu3VoLFiyQJO3evVvOOT322GN67LHHyv1+Bw4cKBPZqrr++utD/52dnR06euVMHrNZGed8IH+5d+bj5N86wWBQt956qx555JFy51u3bl3ptZXKysrShx9+qIcfflidOnXSxRdfrGAwqO7du3u9EXTyP55gMKhAIKB3331XERERYfMXX3xxldd8KnfeeafuuecezZ8/v9KBjI2NDUX3VE7++ZbeX/369dOAAQPKvcxVV11VqTWdKefaGufNm6eBAwcqMzNTDz/8sBo2bKiIiAhNnDhRX3zxRYWvr/T2PfTQQ8rIyCh3pmXLllVa86nExcXp5ptv1uuvv04gKysuLi7sXa7i4mLt37+/zLbk5GT9/PPPXv9YK+OHH37QihUrNH78+DKHLOzatcu8zK5du8r8Zt+9e7eCwWDoKXFycrKcc2revPkZCXhFHT16VMFgsNy93+qWkJCgunXrqqSk5LQ/s8TERG3btk3OuTK/ZD777LPTfp/k5GQtW7ZMBQUFp9yLLO/p9tlao6+FCxeqRYsWWrx4cZnvUd7LB1L5j83PP/889Phr0aKFpBPPVKrr383pHD58uEYefyc751+DtCQnJ4e9fjh9+vSwPcisrCytXbtWy5YtC7uOwsJCHT9+vErrKN3Dcyd99tmp3h2fOnVqma8nT54sSerRo4ekE3twERERGj9+fNj1OufMw4dK+R7mU/rO4clmzJghSUpNTT3l5atDRESE+vTpo0WLFpV76EnpIUKS9Pvf/1779u3TwoULQ9sOHTpkPu39pT59+sg5p/Hjx4f93S/v89jY2LBfxGdrjb7KewyuX79ea9euLXf+rbfeKvMa4oYNG7R+/frQ469hw4bq1q2bXn755bAdDqns7StPRQ7zKe8lrtzcXK1YsaJGHn8nO2/3IIcOHarhw4erT58+uvXWW7V582YtW7ZM8fHxZeYefvhhvf322+rZs6cGDhyolJQUHTx4UFu3btXChQuVm5sbdpmT5eXl6amnngrb3rx5c919993q2rWr/v73v+vYsWNq1qyZ/vvf/yonJ8e8vpycHPXq1Uvdu3fX2rVrNW/ePP3xj38MPZ1NTk7WU089pbFjxyo3N1eZmZmqW7eucnJy9Oabb+ree+/VQw89ZF6/72E+q1ev1qhRo9S3b1+1atVKxcXFWrNmjRYvXqzU1NSwg/YDgYDS0tKq/fTPZ555RqtWrdK1116rYcOGqX379iooKNCmTZv03nvvqaCgQJI0bNgwTZkyRf3799fGjRvVpEkTzZ071+tEgptuukn33HOPXnzxRe3atSv0csiaNWt00003aeTIkZKklJQUvffee/rHP/6hpk2bqnnz5rr22mvPyhp/adasWaFjRH9p9OjR6tmzpxYvXqzevXvrtttuU05OjqZNm6b27duX+2Zby5YtdeONN+pPf/qTjh49qkmTJqlBgwZlXoaaOnWqbrzxRnXs2FHDhg1TixYt9N1332nt2rXau3evNm/ebK61Iof5dOzYUenp6erUqZPi4uK0a9cuzZw5U8eOHauRM67CnPX3zQ3WYT7WITYlJSXuL3/5i4uPj3cxMTEuIyPD7d69O+wwH+ecKyoqcmPHjnUtW7Z0F110kYuPj3c33HCDe+6551xxcfEp11V6qFF5f9LT051zzu3du9f17t3bXXrppa5evXrurrvucvv27Qs71KH0MJ8dO3a4vn37urp167q4uDg3cuRId/jw4bDvvWjRInfjjTe62NhYFxsb69q2betGjBjhPvvss9BMVQ7z2b17t+vfv79r0aKFi46OdlFRUa5Dhw5u3Lhx7ueffw67DyW57OzsU15n6X12ukOjnDtxmM+IESPK/bvvvvvOjRgxwl1++eWuTp06rnHjxi49Pd1Nnz69zNyePXtcr169XExMjIuPj3ejR492S5cuPe1hPs6dODzm2WefdW3btnUXXXSRS0hIcD169HAbN24MzezcudN17drVRUdHO0llHltneo3lKT3Mx/rz9ddfu2Aw6CZMmOASExNdZGSku/rqq90777wTdptLD/N59tln3fPPP+8uv/xyFxkZ6bp06RI6xOyXvvjiC9e/f3/XuHFjV6dOHdesWTPXs2dPt3DhwtBMVQ/zGTdunEtNTXVxcXGudu3armnTpi47O9tt2bLltJc9GwLO8bnYOL0lS5aoZ8+e2rx5szp27FjTywHOivP2NUicXatWrVJ2djZxxK8Ke5AAYGAPEgAMBBIADAQSAAwEEgAMBBIADN5n0pzufwEFAOcL34N32IMEAAOBBAADgQQAA4EEAAOBBAADgQQAA4EEAAOBBAADgQQAA4EEAAOBBAADgQQAA4EEAAOBBAADgQQAA4EEAAOBBAADgQQAA4EEAAOBBAADgQQAA4EEAAOBBAADgQQAA4EEAAOBBAADgQQAA4EEAAOBBAADgQQAA4EEAAOBBAADgQQAA4EEAAOBBAADgQQAA4EEAAOBBAADgQQAA4EEAAOBBAADgQQAA4EEAEPtml7AuaBOnTresxkZGd6zBw8e9J5NSEjwnr3mmmu8Z+Pj471n09LSvGcXLlzoPfvxxx97z65evdp79sCBA96zQGWwBwkABgIJAAYCCQAGAgkABgIJAAYCCQAGAgkABgIJAAYCCQAGAgkAhoBzznkNBgLVvZYaM2bMGO/Zp556qhpX4ic/P997tiKnGp4Ljh496j1bkdMdn332We/Zbdu2ec/i/OSZPfYgAcBCIAHAQCABwEAgAcBAIAHAQCABwEAgAcBAIAHAQCABwEAgAcDApxpKeuedd7xnk5KSvGf/97//ec9W5PTBPXv2eM8mJiZ6z1aX4cOHe89mZmZ6z959993eszExMd6zgwcP9p4tKirynsX5hz1IADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADDwqYY4p6xfv957NiUlpVrW0K9fP+/Z+fPnV8saUL34VEMAqCICCQAGAgkABgIJAAYCCQAGAgkABgIJAAYCCQAGAgkABgIJAAY+1RDVbtasWd6zqamp3rO+p4tVVHx8fLVcL84/7EECgIFAAoCBQAKAgUACgIFAAoCBQAKAgUACgIFAAoCBQAKAgUACgIFTDS9wTZs29Z6dNGmS92yfPn0qsZrTq1XL/3d2MBj0nn300Ue9Z6dMmeI9iwsbe5AAYCCQAGAgkABgIJAAYCCQAGAgkABgIJAAYCCQAGAgkABgIJAAYOBUwwtc//79vWfvvPNO79nq+kTBDz74wHv25Zdf9p5dsWJFZZaDXzn2IAHAQCABwEAgAcBAIAHAQCABwEAgAcBAIAHAQCABwEAgAcBAIAHAwKmGF7h27drV9BK0cOFC79ns7OxqXAlQMexBAoCBQAKAgUACgIFAAoCBQAKAgUACgIFAAoCBQAKAgUACgIFAAoAh4Dw/ni4QCFT3WlAN0tPTvWeXLVtWLWvIz8/3nr3//vu9Z994443KLAfw/lRO9iABwEAgAcBAIAHAQCABwEAgAcBAIAHAQCABwEAgAcBAIAHAQCABwMCphhe4OnXqeM9effXV3rNvv/2292x8fLz3rO8pYJK0Y8cO79knn3zSe7Yin8KI8xOnGgJAFRFIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMHCqIapdamqq9+z48eO9Z7t3716Z5ZzWmDFjvGenTZvmPVtUVFSZ5aAacKohAFQRgQQAA4EEAAOBBAADgQQAA4EEAAOBBAADgQQAA4EEAAOBBAADpxrinBIdHe0926dPH+/ZgQMHes9269bNe/bTTz/1ns3Ozvae3b59u/csKo5TDQGgiggkABgIJAAYCCQAGAgkABgIJAAYCCQAGAgkABgIJAAYCCQAGDjVEL8KDRo08J7t16+f9+zw4cO9Z5OSkrxnR44c6T07c+ZM71mcwKmGAFBFBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMnGoIVEFCQoL37MqVK71nK3Ja4gMPPOA9y2mJJ3CqIQBUEYEEAAOBBAADgQQAA4EEAAOBBAADgQQAA4EEAAOBBAADgQQAA6caAmdJ3bp1vWdnzJjhPXvVVVd5z3bt2tV7Ni8vz3v2fMOphgBQRQQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADLVregEXsgYNGnjPFhYWes+WlJRUYjWoaUVFRd6zR48e9Z5t3bq192xycrL37IV8qqEv9iABwEAgAcBAIAHAQCABwEAgAcBAIAHAQCABwEAgAcBAIAHAQCABwMCphhV05ZVXes+uXr3ae3bFihXesxU5De3AgQPes3PnzvWerYiKnLK2b9++allDRcTFxXnPXnHFFd6zaWlp3rPXX3+996zvJ/RJUp8+fbxn161b5z17oWIPEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMAed5nlIgEKjutZwXUlJSvGdnzpzpPVuRUxgr8rOoyGlo1WXPnj3es7t37/aera7b1qRJE+/ZDh06eM9W189t06ZN3rPp6enesxX5FMbzje/9yx4kABgIJAAYCCQAGAgkABgIJAAYCCQAGAgkABgIJAAYCCQAGAgkABg41bAaVeTT8Zo1a+Y926NHD+/ZiRMnes+eC8630ygroiKfRvnSSy95z06dOtV7Njc313v2QsaphgBQRQQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADJxqeB6KiIjwnm3QoIH37F133eU9e8UVV3jPpqames9269bNe7a6TjXcsWOH92yvXr28Zw8ePOg9m5eX5z2LiuNUQwCoIgIJAAYCCQAGAgkABgIJAAYCCQAGAgkABgIJAAYCCQAGAgkABk41BPCrw6mGAFBFBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMBBIADAQSAAwEEgAMtX0HnXPVuQ4AOOewBwkABgIJAAYCCQAGAgkABgIJAAYCCQAGAgkABgIJAAYCCQCG/wOlVLVSTi5DPgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}